
|   Lab 1   |

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

boston = pd.read_csv("BostonHousing.csv")
boston

boston.shape

boston.columns

 df= pd.DataFrame(boston)
df.columns = boston.columns
df.columns

df

 df.head

boston["medv"].shape

df['price'] = boston["medv"]
df['price']
df

df.head()
df = df.drop(["medv"], axis=1)

 df.describe()

df

df.info()

df.describe()

x = boston
y = boston["medv"]
x = x.drop(["medv", "price"], axis=1)
x

 from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain, ytest = train_test_split(x,y,test_size=0.2, random_state=42)
print('xtrain shape:', xtrain.shape)
print('xtest shape:', xtest.shape)
print('ytrain shape:', ytrain.shape)
print('ytest shape:', ytest.shape)


from sklearn.linear_model import LinearRegression
lr= LinearRegression()
lr.fit(xtrain, ytrain)
y_pred = lr.predict(xtest)


plt.scatter(ytest,y_pred, c = 'green')
plt.xlabel("Price:in 1000's")
plt.ylabel("Predicted Value")
plt.title("true Values vs Predicted Values:Linear Regression")
plt.show()


from sklearn.metrics import mean_squared_error, mean_absolute_error
mse = mean_squared_error(ytest, y_pred)
mae = mean_absolute_error(ytest, y_pred)
print("MSE",mse)
print("MAE",mae)

pip install tensorflow

import tensorflow as tf

model = tf.keras.Sequential([
tf.keras.layers.Dense(64, activation = "relu", input_shape = (xtrain.shape[1],)),
tf.keras.layers.Dense(32, activation = "relu"),
12
tf.keras.layers.Dense(32, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(1)
])
#compile the model
# model.compile(optimize = "adam", loss = "mean_squared_error")
model.compile(
optimizer="adam", loss="mean_squared_error",
)
#training the model
model.fit(xtrain, ytrain, epochs = 100, batch_size = 32, validation_data =(xtest, ytest))

loss = model.evaluate(xtest, ytest)
print(f"MSE of test data = {loss}")



------------------------------------------------------------------------------

Lab 2 

 import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("letter-recognition.data")
df

df.shape

df.sample(30)

from sklearn.preprocessing import LabelEncoder

lab = LabelEncoder()

df['T'] = lab.fit_transform(df['T'])

 df

x = df.drop(["T"], axis=1)

x

 y= df["T"]

y

sns.countplot(x=y)

y.value_counts()

from sklearn.preprocessing import MinMaxScaler
sc=MinMaxScaler()

x_scale = sc.fit_transform(x)
x_scale


from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x_scale, y,random_state=42,test_size=0.5)

x_train.shape

 x_test.shape

 y_train.shape

y_test.shape

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()

 knn.fit(x_train, y_train)

y_train= knn.predict(x_test)

 y_pred=knn.predict(x_test)
y_pred

from sklearn.metrics import accuracy_score,classification_report

accuracy_score(y_test,y_pred)

print(classification_report(y_test, y_train))

pip install tensorflow

import tensorflow as tf

model = tf.keras.Sequential([
tf.keras.layers.Dense(64, activation = "relu", input_shape = (x_train.shape[1],)),
tf.keras.layers.Dense(32, activation = "relu"),
tf.keras.layers.Dense(32, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(16, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(4, activation = "relu"),
tf.keras.layers.Dense(1)
])
#compile the model
# model.compile(optimize = "adam", loss = "mean_squared_error")
model.compile(
optimizer="adam", loss="mean_squared_error",
)
#training the model
model.fit(x_train, y_train, epochs = 100, batch_size = 32, validation_data =(x_test, y_test))


#evaluate the model
loss = model.evaluate(x_test, y_test)
print(f"MSE of test data = {loss}")


-----------------------------------------------------------------------------------------------------

Lab 3 (2a)


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from keras.datasets import imdb
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)

data = np.concatenate((X_train, X_test), axis=0)

label = np.concatenate((y_train, y_test), axis=0)

 X_train.shape

X_test.shape

 y_test.shape

y_train.shape

print("Review is ",X_train[0]) # series of no converted word to vocabulory␣
↪associated with index
print("Review is ",y_train[0])


 vocab=imdb.get_word_index() # Retrieve the word index file mapping words to␣
↪indices
print(vocab)

y_train

 y_test


 def vectorize(sequences, dimension=10000):
results = np.zeros((len(sequences), dimension))
for i, sequence in enumerate(sequences):
results[i, sequence] = 1
return results
test_x = data[:10000]
test_y = label[:10000]
train_x = data[10000:]
train_y = label[10000:]

test_x.shape

 test_y.shape

train_x.shape

train_y.shape

print("Categories:", np.unique(label))
print("Number of unique words:", len(np.unique(np.hstack(data))))

length = [len(i) for i in data]
print("Average Review length:", np.mean(length))
print("Standard Deviation:", round(np.std(length)))


print("Label:", label[0])

print("Label:", label[1])

print(data[0])

index = imdb.get_word_index()

reverse_index = dict([(value, key) for (key, value) in index.items()])
decoded = " ".join( [reverse_index.get(i - 3, "#") for i in data[0]] )


print(decoded)


pip install seaborn

import seaborn as sns

data = vectorize(data)
label = np.array(label).astype("float32")
labelDF = pd.DataFrame({'label': label})
sns.countplot(x='label', data=labelDF)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20,random_state=1)

X_train.shape
X_test.shape

from keras.utils import to_categorical
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(50, activation = "relu", input_shape=(10000, )))
model.add(layers.Dropout(0.3, noise_shape=None, seed=None))
model.add(layers.Dense(50, activation = "relu"))
model.add(layers.Dropout(0.2, noise_shape=None, seed=None))
model.add(layers.Dense(50, activation = "relu"))
model.add(layers.Dense(1, activation = "sigmoid"))
model.summary()

import tensorflow as tf
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
model.compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = ["accuracy"]
)
from sklearn.model_selection import train_test_split
results = model.fit(
X_train, y_train,
epochs= 2,
batch_size = 500,
validation_data = (X_test, y_test),
callbacks=[callback]
)
print(np.mean(results.history["val_accuracy"]))
score = model.evaluate(X_test, y_test, batch_size=500)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

print(results.history.keys())
plt.plot(results.history['accuracy'])
plt.plot(results.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.plot(results.history['loss'])
plt.plot(results.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


---------------------------------------------------------------------------------------------------


Lab 4 (3)

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
import numpy as np

(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()

plt.imshow(x_train[1])

plt.imshow(x_train[0])

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

model = keras.Sequential([
keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
keras.layers.MaxPooling2D((2,2)),
keras.layers.Dropout(0.25),
keras.layers.Conv2D(64, (3,3), activation='relu'),
keras.layers.MaxPooling2D((2,2)),
keras.layers.Dropout(0.25),
keras.layers.Conv2D(128, (3,3), activation='relu'),
keras.layers.Flatten(),
keras.layers.Dense(128, activation='relu'),
keras.layers.Dropout(0.25),
keras.layers.Dense(10, activation='softmax')
])
model.summary()
Model: "sequential"

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',␣
↪metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test,␣
↪y_test))
test_loss, test_acc = model.evaluate(x_test, y_test)
print('Test accuracy:', test_acc)










